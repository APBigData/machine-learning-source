---
title: "Neurale netwerken"
author: "David D'Haese"
output: html_document
---

```{r include=FALSE}
library(knitr)
library(data.table)
library(reticulate)
library(magrittr)
library(e1071)
library(latex2exp)

opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, warning = FALSE)
source("r/md_tools.r")
dyad <- readRDS("r/Palette_5YR_5B_Dyad.RDS")
palette(dyad[c(9, 20, 66)])
```

# Artificiële Neurale Netwerken (ANN's)

## Inleiding tot ANN's

Een artificieel neuraal netwerk (ANN) kan gezien worden als een samenstelling van individuele perceptronen zodat de uitvoer van de ene perceptron de invoer van een andere wordt. Maar de neuronen worden niet kris-kras door elkaar geplaatst, maar worden georganiseerd in zogenaamde lagen (eng: _layers_). In een feed-forward netwerk resulteert dit in een [gerichte acyclische graaf](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (eng: _directed acyclic graph_) van noden of neuronen. Laten we een eenvoudig voorbeeld bekijken. In het hoofdstuk rond de perceptron hebben we _Iris setosa_ bloemen weten te onderscheiden van andere soorten op basis van twee eigenschappen, namelijk de lengte en breedte van de kelkbladeren. Als we nu eens proberen de drie soorten te onderscheiden. Met één perceptron lukt het niet, want herinner je dat een perceptron enkel lineair gescheiden punten-wolken kan onderscheiden en een lijn zal telkens maar leiden tot een uitkomst met twee mogelijke categorieën.

Doordat we nu meerdere uitkomsten hebben moeten we voor elke uitkomst een node (neuron, perceptron) voorzien. De invoer van deze 3 uitkomst-noden ("neuronen in de uitvoer laag", eng: _output-layer_) is verbonden met de uitvoer van de 2 invoer-laag neuronen (omdat er twee eigenschappen zijn). Zo ziet het nieuwe netwerk er uit:

```{r nn-iris-0, fig.cap="(ref:nn-iris-0)"}
library(neuralnet)

nn = neuralnet(Species ~ Sepal.Length + Sepal.Width,
  data = iris, hidden = 0, linear.output = TRUE) 
nn %>% plot(rep = "best")
```

(ref:nn-iris-0) Een ANN om op basis van twee eigenschappen van de kelkbladeren van iris-bloemen drie soorten (_setosa_, _vericolor_ en _virginica_) te onderscheiden. Dit netwerk heeft enkel een invoer- en een uivoerlaag. De 'Error' geeft de nauwkeurigheid weer. 'Steps' het aantal doorlopen epochs.

Uit het bovenstaand netwerk kunnen we het volgende aflezen:

- Het bestaat uit twee lagen, een invoer-laag (eng: _input layer_) en een uitvoer-laag (eng: _output layer_)
- Het leeralgoritme heeft 2251 epochs nodig gehad om tot een stabiele oplossing te komen
- De totale fout die gemaakt wordt, is recht evenredig met $\approx 26.4\,mm^2$
- Of een bloem van de soort _Iris setosa_ is heeft te maken met zowel de lengte als de breedte van het kelkblad, zij het in omgekeerde verhouding
- Vooral de breedte van het kelkblad bepaald of de bloem een _versicolor_ is terwijl het eerder de lengte is dat bepaald of het een _virginica_ is.

De resultaten van bovenstaand netwerk kunnen als matrix $\mathbf{\theta}$ worden weergegeven. Laten we deze matrix voor de volledigheid eens bekijken:

```{r iris-o-results, results="asis", echo=FALSE}
nn$weights[[1]][[1]] %>% write_matex(prefix = "\\theta=")
```

De totale fout waarvan hierboven sprake is, is evenredig met de som van de kwadraten van de deviaties tussen $y$ en $\hat{y}$ (Formule \@ref(eq:kwadratensom)).

Behalve een invoer-laag en een uitvoer-laag, kan een netwerk ook worden opgebouwd uit tussenliggende _verborgen lagen_ (eng: _hidden layers_). Figuur \@ref(fig:nn-iris-1) laat zien wat er gebeurt indien we aan het Iris-netwerk een verborgen laag toevoegen met daarin 3 noden.

```{r nn-iris-1, fig.cap="(ref:nn-iris-1)"}
library(neuralnet)

nn = neuralnet(Species ~ Sepal.Length + Sepal.Width,
  data = iris, hidden = 3, linear.output = TRUE) 
nn %>% plot(rep = "best")
```

(ref:nn-iris-1) Idem als voor Figuur \@ref(fig:nn-iris-0), maar nu met toevoeging van een verborgen laag met 3 neuronen.

Merk op dat de totale fout $E$ nu kleiner is geworden, maar dat betekent nog niet dat dit een goed idee is om de extra verborgen laag inderdaad toe te voegen.

```{exercise goed-idee-tussenlaag}
Het toevoegen van een extra verborgen laag in ons model zorgt ervoor dat de totale fout op de Iris dataset verkleint. Maar dat betekent nog niet dat het een goed idee is om een laag toe te voegen. Waarom niet? Leg uit in eigen woorden.
```

## Feed-forward ANNs (FF-ANNs)

We geen nu iets formeler moeten zijn in wat de regels rond een ANN zijn en hoe een ANN aan zijn oplossing komt. Om te beginnen leggen we de regels vast voor de noden en connectoren (pijlen) een FF-ANN:

```{definition ff-ann-noden}
__Noden__:

- De _invoer-laag_ is verplicht en bevat één node voor elke feature
- De _uitvoer-laag_ is ook verplicht en bevat één node voor _elke categorie van de uitkomst_
- Verborgen lagen zijn optioneel
- Het aantal noden in de verborgen lagen is bij realistische situaties met vele features meestal veel kleiner dan het aantal features zodat de invoer a.h.w. gecomprimeerd wordt
- Elke laag, behalve de uitvoer-laag, heeft een speciale afwijking-node (eng: _bias-node_) die de constante waarde 1 bevat
```

```{definition ff-ann-connectoren}
__Connectoren__:

- In een FF-ANN mogen connectoren niet twee noden van dezelfde laag verbinden
- Bovendien moet de zin van de connectoren lopen van invoer-laag naar uitvoerlaag en nooit andersom
- Het is niet zo dat alle node van de ene laag moeten verbonden zijn met alle noden van de volgende laag
```

Hoe werkt ANN nu eigenlijk? Wel, als je het perceptron begrijpt is er eigenlijk niet veel aan. De waarden van de invoer-laag zijn uiteraard gekend. Voor elke andere laag in het netwerk bekom je de nieuwe waarden door de waarden uit de voorgaande laag te vermenigvuldigen (via inwendig product) met de bijhorende gewichten:

\begin{equation}
  z_\ell=\mathbf{x_\ell}\cdot\mathbf{\theta_\ell} \\
  \hat{y}_\ell=t(z_\ell) \\
  \ell\subset\{2, 3,..,L\}
  (\#eq:layer-matrix-multiplication)
\end{equation}

Komt het FF-ANN leeralgoritme aan de laatste laag van het netwerk aan, dan wordt de geschatte uitkomst $\hat{y}$ vergeleken met de werkelijke uitkomst $y$ en op basis daarvan de gewichten aangepast.

## Types neuronen

Tot hier toe hebben we uitsluitend lineaire neuronen besproken, maar om complexere problemen te kunnen afhandelen was er een manier nodig om een niet-lineaire respons in de neuronen in te bouwen. Dit doen we door van de transformatie functie $t(z)$ een niet-lineaire functie te maken. Het type transformatie-functie bepaald dan ook het type neuron. Er zijn vele types lineaire en niet-lineaire neuronen, maar enkel de meest courante worden hieronder visueel weergegeven.

```{r sigmoid-neuron, fig.asp=1, echo=FALSE, fig.cap="(ref:sigmoid-neuron)"}
par(mfrow = c(2, 2))
plot(0, type = "n", xlim = c(-10, 10), ylim = 0:1,
  xlab = TeX("$z$"), ylab = TeX("$\\hat{y}$"), main = "Sigmoid")
abline(v = 0)
axis(2, pos = 0, labels = FALSE)
curve(1/(1 + exp(-x)), add = TRUE, col = 1, lwd = 2)

plot(0, type = "n", xlim = c(-10, 10), ylim = c(-1, 1),
  xlab = TeX("$z$"), ylab = TeX("$\\hat{y}$"), main = "Tanh")
abline(v = 0, h = 0)
curve(tanh(x), add = TRUE, col = 1, lwd = 2)

plot(0, type = "n", xlim = c(-10, 10), ylim = c(0, 10),
  xlab = TeX("$z$"), ylab = TeX("$\\hat{y}$"), main = "ReLU")
abline(v = 0)
axis(2, pos = 0, labels = FALSE)
curve(pmax(0, x), add = TRUE, col = 1, lwd = 2)

plot(0, type = "n", xlim = c(-10, 10), ylim = 0:1,
  xlab = TeX("$z$"), ylab = TeX("$\\hat{y}$"), main = "Heaviside")
abline(v = 0, lty = 3)
segments(c(-10, 0), 0:1, c(0, 10), 0:1, col = 1, lwd = 2)
points(0, 0, pch = 21, col = 1, bg = "white", cex = 2, lwd = 2)
points(0, 1, pch = 21, col = 1, bg = 1, cex = 2)
```

(ref:sigmoid-neuron) Vier neuron-types gebaseerd op hun transformatie- functies $t(z)$. De functie voor de sigmoïd is $\frac{1}{1+e^{-z}}$, die van het _tanh_ neuron type is uiteraard $tanh(x)$ en die voor het Restricted Linear Unit neuron (ReLU) is $max(0, z)$.

Sommige restricties gelden voor alle neuronen binnen een laag. Zo is er de zogenaamde _softmax_-type uitvoer-laag. Hierbij stellen de neuronen binnen die laag een kansverdeling voor en moet bijgevolg de som van de neuronen op exact 1 uitkomen:

\begin{equation}
  \hat{y}_\ell=\frac{e^{z_\ell}}{\sum_{\ell}{e^{z_\ell}}}
  (\#eq:softmax)
\end{equation}

## Backpropagation

Tijdens het trainen van meerlagige ANNs maken we gebruik van het _backpropagation algoritme_ (zie @rumelhart1986) een voorbeeld van [dynamisch programmeren](https://en.wikipedia.org/wiki/Dynamic_programming) (eng: _dynamic programming_). Hieronder zijn de stappen uiteengezet:

1. Na de initialisatie, waarbij beginwaarden aan de parameters worden toegekend, doorlopen we de lagen één voor één af te beginnen bij de eerste laag na de invoerlaag. Bij elke laag berekenen we $x_{k+1}=t_k(f(x_k)), k\subset\{1,..,K-1\}$, waarbij $t-k$ de verliesfunctie is voor die bepaalde laag.
2. Nu pas beginnen we met de backpropagation. Te vertrekken deze keer van de uitvoerlaag, berekenen we de restterm $\varepsilon$.
3. Nu berekenen we voor elke laag, van laatste naar tweede, de afgeleiden $\frac{\partial{E_n}}{\partial\theta_{n-1}}$ (eng: _error  derivatives_) die aangeeft hoeveel de restterm van een laag verandert naarmate de parameters van de neuronen uit de vorige laag veranderen
4. We vullen nu de leersnelheid in en lossen de zogenaamde modificatie-formule (eng: _modification formula_) op om de nieuwe parameter waarden te berekenen en we kunnen voort naar de volgende epoch

Zie de blog van Jay Prakash voor een uitgewerkt voorbeeld en de precies berekening van de afgeleiden (@Prakash2017).

## Werkstroom deep learning

Figuur \@ref(fig:deep-learning-workflow) vat samen hoe de werkstroom van een ANN eruit ziet. Het is een gal complexe werkstroom maar desalniettemin erg belangrijk. Dankzij deze werkstroom weten we hoe lang we moeten blijven trainen, i.e. hoeveel epoch we moeten doorlopen alvorens we kunnen stoppen. Het begint met het verzamelen van gegevens. Op basis van de _structuur van de gegevens_ ontwerpen we de architectuur van ons neuraal netwerk. De data wordt ondertussen gesplitst in een training-, validatie- en test-set (zie [betrokken paragraaf](#training--validatie-en-test-set) voor meer info). We beginnen dan met de training-set. Na de eerste epoch onderzoeken we of de restterm kleiner wordt of niet.

```{r deep-learning-workflow, fig.cap="(ref:deep-learning-workflow)"}
include_graphics("img/deep-learning-workflow.svg")
```

(ref:deep-learning-workflow) De typische werkstroom voor het afhandelen van deep-learning ANN. Meer uitleg in de tekst. GEbaseerd op de werkstroom in figuur 2-14 van @buduma.

TODO: http://neuralnetworksanddeeplearning.com/chap1.html

